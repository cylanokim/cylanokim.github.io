--- 
title: "LoRAë¥¼ ì´ìš©í•œ LLM Fine Tunning"
description: LLMì„ ì´ìš©í•œ ê°„ë‹¨í•˜ê²Œ íŒŒì¸ íŠœë‹ì„ í•´ë³´ì.(PEFT) 
author: cylanokim
date: 2025-12-26 12:00:00 +0800
categories: [LLM]
tags: [LLM, LoRA, FineTuning]
pin: true
math: true
mermaid: true
---


## Step 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```python
!pip install transformers==4.42.4
!pip install datasets
!pip install git+https://github.com/huggingface/peft.git@679bcd8777fxxxxxxxxx
```

```python
import datasets, torch
```

- transformers : Hugging Faceì—ì„œ ë§Œë“  ë¼ì´ë¸ŒëŸ¬ë¦¬. 
- datasets : ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì„ ë¹ ë¥´ê³ , ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ í‘œì¤€ ë„êµ¬
- huggingface/peft.git@6... : Huggingface/peftì˜ íŠ¹ì • ì»¤ë°‹ ë²„ì „ìœ¼ë¡œ ì§ì ‘ ì„¤ì¹˜
- @67xxx : ê¹ƒ ì»¤ë°‹ í•´ì‹œ â†’ ì¬í˜„ì„± í™•ë³´ 

## Step 1. Load AutoModel (OPT-2.7B)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
model_id = "facebook/opt-2.7b"
# Model Load
model_opt = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype="auto", device_map="auto", cache_dir="/home/ms/hf_cache/hub")
# Tokenizer Load
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir="/home/ms/hf_cache")
```

- ëª¨ë¸ ì •ë³´ : https://huggingface.co/facebook/opt-2.7b
- OPT : Open Pretrained Transformer. Metaê°€ ê³µê°œí•œ ëª¨ë¸. 27ì–µê°œ. Decoder only
- ëª¨ë¸ í´ë˜ìŠ¤ ë¹„êµ

| ëª¨ë¸   | í´ë˜ìŠ¤                     | íŠ¹ì§•              |
| ---- | ----------------------- | --------------- |
| BERT | `AutoModelForMaskedLM`  | ì–‘ë°©í–¥, [MASK] ì˜ˆì¸¡  |
| T5   | `AutoModelForSeq2SeqLM` | Encoder-Decoder |
| GPT  | `AutoModelForCausalLM`  | ë‹¨ë°©í–¥, ìƒì„±         |

- device_map :ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–¤ ë””ë°”ì´ìŠ¤(GPU/CPU)ì— ì˜¬ë¦´ì§€ ì •í•˜ëŠ” ë°°ì¹˜ ì§€ë„ 
    - device_map="auto" : GPU VRAM ìš©ëŸ‰ê³¼ ëª¨ë¸ íŒŒë¼ë¯¸í„° í¬ê¸°ë¥¼ ê³ ë ¤í•´ ë ˆì´ì–´ ë‹¨ìœ„ë¡œ ìµœì  ë°°ì¹˜ë¥¼ ì§„í–‰í•œë‹¤. 
    - device_map="cuda" : GPUê°€ ë„‰ë„‰í• ë•Œ

```text
embed_tokens        â†’ cuda:0
decoder.layers.0-15 â†’ cuda:0
decoder.layers.16-23â†’ cpu
lm_head             â†’ cuda:0
```

- cache_dir : Hugging Faceê°€ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ / í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ë¡œì»¬ ê²½ë¡œ

### ì°¸ê³  í•¨ìˆ˜
```python
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )
```

## Step 2. LoRA Implementation

```python
from peft import LoraConfig, get_peft_model
# Setting for LoRA PEFT (fine-tuning QKV projection weight)
config = LoraConfig(
    r=4, # LoRA rank [2,4,8,16,64,...]
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj"], # target modules ["fc1", "fc2", "q_proj", "k_proj", "v_proj"]
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA to OPT Pre-Trained Model
model_opt.gradient_checkpointing_enable()
model_opt = get_peft_model(model_opt, config)
print_trainable_parameters(model_opt)
```

```text
trainable params: 1966080 || all params: 2653562880 || trainable%: 0.07409208256636451
```

## Step 3. Pre-Process & Tokenize Instruction Dataset

```python
!pip uninstall -y datasets
!pip install "datasets<4.0.0"
```


```python
from datasets import load_dataset

data = load_dataset("piqa", split="train[:10%]").select(range(100))
print(data.column_names)
```

```text
['goal', 'sol1', 'sol2', 'label']
```

- PIQA : Physical Interation Question Answering ë°ì´í„° ì…‹

| column  | ì˜ë¯¸                  |
| ------- | ------------------- |
| `goal`  | ì‚¬ëŒì´ í•˜ê³  ì‹¶ì€ í–‰ë™ ì„¤ëª…     |
| `sol1`  | ì²« ë²ˆì§¸ ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•      |
| `sol2`  | ë‘ ë²ˆì§¸ ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•      |
| `label` | ì •ë‹µ ì¸ë±ìŠ¤ (`0` ë˜ëŠ” `1`) |

```text
label = 0  â†’ sol1 ì´ ì •ë‹µ
label = 1  â†’ sol2 ì´ ì •ë‹µ
```

- ë°ì´í„° êµ¬ì¡°

<div style="text-align: center;">
  <img src="/assets/img/piqa_dataset.PNG" alt="piqa" width="500">
</div>


### Pre-Processing (concat goal and solution)

```python
def add_sol_with_label(example):
  sentence = example[column_names[0]] + " "
  answer = example[column_names[1]] if example["label"] == 0 else example[column_names[2]]

  example["sentence"] = sentence + answer
  return example
```

```python
# Pre-Processing PIQA train dataset
updated_data = data.map(add_sol_with_label)
updated_data = updated_data.remove_columns("goal")
updated_data = updated_data.remove_columns("label")
updated_data = updated_data.rename_column("sentence", "goal")
data = updated_data

# Tokenize
data = data.map(lambda samples:tokenizer(samples["goal"]), batched=True)
```

<div style="text-align: center;">
  <img src="/assets/img/piqa_dataset2.PNG" alt="piqa" width="500">
</div>

## Step 4. Text Generation Before Fine-Tuning

```python
text = "What is SOH in semiconductor manufacturing process?"
# Set max sequence length
max_token_number = 50

# Tokenize input sequence
inputs = tokenizer(text, return_tensors="pt").to("cuda:0")

# Text Generation (model inference)
with torch.no_grad():
    outputs_opt = model_opt.generate(**inputs, max_tokens=max_token_number)
print(tokenizer.decode(outputs_opt[0], skip_special_tokens=True))
```

```text
What is SOH in semiconductor manufacturing process?
SOH is a term used to describe the amount of oxygen in the atmosphere. It is measured in parts per million (ppm).
SOH is a measure of the amount of oxygen in the atmosphere. It is measured in parts
```

íŒŒì¸ íŠœë‹ì„ ìœ„í•œ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ë³´ì. ì•„ë˜ êµ¬ì¡°ëŠ” **instruction tuning**ì—ì„œ ì „í˜•ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ í˜•íƒœì´ë‹¤. ì—¬ê¸°ì„œ instructionì€  ëª¨ë¸ì—ê²Œ ì–´ë–¤ ì—­í• ì„ ìˆ˜í–‰í•˜ë¼ê³  ì§€ì‹œí•˜ëŠ” ìƒìœ„ ëª…ë ¹ì„ ì˜ë¯¸í•œë‹¤. ì¼ì¢…ì˜ system prompt ì—­í• ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ëª¨ë¸ì´ ì–´ë–¤ ì¢…ë¥˜ì˜ ì§ˆë¬¸ì„ ì–´ë–»ê²Œ ë‹µí•´ì•¼ í•˜ëŠ”ì§€ë¥¼ í•™ìŠµí•˜ê²Œ í•œë‹¤. ì´ë•Œ Decoder only ëª¨ë¸ì˜ ê²½ìš° `instruction + input + output`ì€ í•˜ë‚˜ì˜ ì—°ì†ëœ í† í° ì‹œí€€ìŠ¤ë¡œ ëª¨ë¸ì— ë“¤ì–´ê°„ë‹¤. 

```python
data_etch = {
    "instruction": [
        "Tell me about the films used in the semiconductor process",
        "Tell me about the films used in the semiconductor process",
    ],
    "input": [
        "What is SION in semiconductor manufacturing process?",
        "What is SOH in semiconductor manufacturing process?",
    ],
    "output": [
        "In semiconductor manufacturing, SiON refers to silicon oxynitride.",
        "In semiconductor manufacturing, SOH(Spin On Hardmask) is a high carbon-containing polymer material.",
    ],
}
from datasets import Dataset
# Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
dataset = Dataset.from_dict(data_etch)
```

```python
def preprocess(examples):
    texts = [
        f"Instruction: {inst}\nInput: {inp}\nAnswer: {out}"
        for inst, inp, out in zip(examples["instruction"], examples["input"], examples["output"])
    ]
    model_inputs = tokenizer(texts, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    return model_inputs

tokenized = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)
```

## Step 5. HuggingFace Trainer Setting for LLM Fine-Tuning 

```python
import transformers

tokenizer.pad_token = tokenizer.eos_token

# Fine-Tuning Setting data -> tokenized
trainer = transformers.Trainer(
    model=model_opt,
    train_dataset=tokenized,
     args=transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        max_steps=100,
        learning_rate=1e-4,
        fp16=True,
        logging_steps=10,
        output_dir="outputs",
        report_to='none'
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model_opt.config.use_cache = False  # silence the warnings. Please re-enable for inference!

# Let's Fine-Tuning ğŸ”¥ğŸ”¥ğŸ”¥
trainer.train()
```
- `tokenizer.pad_token = tokenizer.eos_token`: GPT/LLaMA ê³„ì—´ì€ pad_tokenì´ ì—†ìŒ. 
- `mlm=False` : GPT ê³„ì—´, `mlm=True` : BERT ê³„ì—´(Masked LLM) 

## Step 6. Text Generation after Fine-Tuning ğŸ“šğŸ“š

```python
text = "What is SOH in semiconductor manufacturing process?"
# Set max sequence length
max_token_number = 50

# Tokenize input sequence
inputs = tokenizer(text, return_tensors="pt").to("cuda:0")

# Text Generation (model inference)
with torch.no_grad():
    outputs_opt = model_opt.generate(**inputs, max_tokens=max_token_number)
print(tokenizer.decode(outputs_opt[0], skip_special_tokens=True))
```

```text
What is SOH in semiconductor manufacturing process?
Spin-On Hardmask. In semiconductor manufacturing, it refers to silicon oxynitride.
```

